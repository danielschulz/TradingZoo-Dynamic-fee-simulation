{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare env\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from custom_trading_env import TradingEnv\n",
    "from utils import device\n",
    "import DQNTradingAgent.dqn_agent as dqn_agent\n",
    "from custom_hyperparameters import hyperparams\n",
    "from arguments import argparser\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.agent_num = 1\n",
    "        self.device_num = 0\n",
    "        self.save_num = 1\n",
    "        self.risk_aversion = 1.\n",
    "        self.n_episodes = 1000\n",
    "        self.i = 1.\n",
    "        self.fee = .0001\n",
    "        self.render = False\n",
    "\n",
    "args = args()\n",
    "# device_num, save_num, risk_aversion, n_episodes, fee\n",
    "\n",
    "device = torch.device(\"cuda:{}\".format(args.device_num))\n",
    "dqn_agent.set_device(device)\n",
    "\n",
    "save_location = 'saves/{}'.format(args.save_num)\n",
    "\n",
    "if not os.path.exists(save_location):\n",
    "    os.makedirs(save_location)\n",
    "\n",
    "save_interval  = 200\n",
    "print_interval = 1\n",
    "\n",
    "n_episodes   = args.n_episodes\n",
    "sample_len   = 480\n",
    "obs_data_len = 192\n",
    "step_len     = 1\n",
    "fee          = args.fee\n",
    "sell_at_end  = False\n",
    "\n",
    "risk_aversion_multiplier = 0.5 + args.risk_aversion / 2\n",
    "\n",
    "n_action_intervals = 5\n",
    "\n",
    "init_budget = 1\n",
    "\n",
    "torch.save(hyperparams, os.path.join(save_location, \"hyperparams.pth\"))\n",
    "\n",
    "df = pd.read_hdf('dataset/binance_data_train.h5', 'STW')\n",
    "df.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-24 13:54:57,722] Making new env: custom_trading_env\n"
     ]
    }
   ],
   "source": [
    "env = TradingEnv(custom_args=args, env_id='custom_trading_env', obs_data_len=obs_data_len, step_len=step_len, sample_len=sample_len,\n",
    "                     df=df, fee=fee, initial_budget=1, n_action_intervals=n_action_intervals, deal_col_name='c',\n",
    "                     sell_at_end=sell_at_end,\n",
    "                     feature_names=['o', 'h','l','c','v',\n",
    "                                    'num_trades', 'taker_base_vol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm = nn.LSTM(3,6,batch_first = True)\n",
    "inputs = torch.randn(5,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(3,6,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, hidden = gru(inputs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 6])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden = (torch.zeros(1,5,6),torch.zeros(1,5,6)) #(time_step,batch_size, out_shape)\n",
    "# lstm(inputs, hidden )\n",
    "out, hidden = lstm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 6])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape # batch, timestep, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 6])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[1].shape # 1, timestep, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Most part of the implementation from http://nlp.seas.harvard.edu/2018/04/03/attention\"\"\"\n",
    "    def __init__(self, h, d_model):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.attn = None\n",
    "\n",
    "        self.query_linear  = nn.Linear(d_model, d_model)\n",
    "        self.key_linear    = nn.Linear(d_model, d_model)\n",
    "        self.value_linear  = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query = self.query_linear(query).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        key   = self.key_linear(key).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        value = self.value_linear(value).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = self.attention(query, key, value)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.output_linear(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        \"Compute 'Scaled Dot Product Attention'\"\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test attention\n",
    "class AttentionFFLayer(nn.Module):\n",
    "        def __init__(self, in_out_size, h=8):\n",
    "            assert in_out_size % h == 0, \"The input size must be divisible by the number of attention heads `h`.\"\n",
    "            super().__init__()\n",
    "            self.in_out_size = in_out_size\n",
    "            self.h = h\n",
    "\n",
    "            self.norm0 = nn.LayerNorm(in_out_size)\n",
    "            self.attn  = MultiHeadedAttention(h, in_out_size)\n",
    "            self.norm1 = nn.LayerNorm(in_out_size)\n",
    "            self.fc    = nn.Sequential(\n",
    "                nn.Linear(in_out_size, in_out_size * 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_out_size * 4, in_out_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.attn(*([self.norm0(x)]*3)) # same input for `query`, `key`, and `value`\n",
    "            x = x + self.fc(self.norm1(x))\n",
    "            return x\n",
    "\n",
    "class FinalFFAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_out_size, h=8):\n",
    "        assert in_out_size % h == 0, \"The input size must be divisible by the number of attention heads `h`.\"\n",
    "        super().__init__()\n",
    "        self.in_out_size = in_out_size\n",
    "        self.h = h\n",
    "\n",
    "        self.norm0 = nn.LayerNorm(in_out_size)\n",
    "        self.fc    = nn.Linear(in_out_size, in_out_size)\n",
    "        self.norm1 = nn.LayerNorm(in_out_size)\n",
    "        self.attn  = MultiHeadedAttention(h, in_out_size)\n",
    "        self.norm2 = nn.LayerNorm(in_out_size)\n",
    "\n",
    "    def forward(self, memory):\n",
    "        assert memory.dim() == 3\n",
    "        x = memory[:, -1:]\n",
    "        x = x + self.fc(self.norm0(x))\n",
    "        x = x + self.attn(self.norm1(x), memory, memory)\n",
    "        return self.norm2(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQNTradingAgent.default_hyperparameters import SEED, N_ATOMS, INIT_SIGMA, LINEAR, FACTORIZED\n",
    "from DQNTradingAgent.default_hyperparameters import SEED, BUFFER_SIZE, BATCH_SIZE, START_SINCE,\\\n",
    "                                    GAMMA, T_UPDATE, TAU, LR, WEIGHT_DECAY, UPDATE_EVERY,\\\n",
    "                                    A, INIT_BETA, P_EPS, N_STEPS, V_MIN, V_MAX,\\\n",
    "                                    CLIP, N_ATOMS, INIT_SIGMA, LINEAR, FACTORIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, initial_sigma=INIT_SIGMA, factorized=FACTORIZED):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.initial_sigma = initial_sigma\n",
    "        self.factorized = factorized\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.noisy_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "            self.noisy_bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            self.register_parameter('noisy_bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.noise = True\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.factorized:\n",
    "            sqrt_input_size = math.sqrt(self.weight.size(1))\n",
    "            bound = 1 / sqrt_input_size\n",
    "            nn.init.constant_(self.noisy_weight, self.initial_sigma / sqrt_input_size)\n",
    "        else:\n",
    "            bound = math.sqrt(3 / self.weight.size(1))\n",
    "            nn.init.constant_(self.noisy_weight, self.initial_sigma)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "            if self.factorized:\n",
    "                nn.init.constant_(self.noisy_bias, self.initial_sigma / sqrt_input_size)\n",
    "            else:\n",
    "                nn.init.constant_(self.noisy_bias, self.initial_sigma)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.noise:\n",
    "            if self.factorized:\n",
    "                input_noise  = torch.randn(1, self.noisy_weight.size(1), device=self.noisy_weight.device)\n",
    "                input_noise  = input_noise.sign().mul(input_noise.abs().sqrt())\n",
    "                output_noise = torch.randn(self.noisy_weight.size(0), device=self.noisy_weight.device)\n",
    "                output_noise = output_noise.sign().mul(output_noise.abs().sqrt())\n",
    "                weight_noise = input_noise.mul(output_noise.unsqueeze(1))\n",
    "                bias_noise = output_noise\n",
    "            else:\n",
    "                weight_noise = torch.randn_like(self.noisy_weight)\n",
    "                bias_noise = None if self.bias is None else torch.randn_like(self.noisy_bias)\n",
    "\n",
    "            if self.bias is None:\n",
    "                return F.linear(\n",
    "                           input,\n",
    "                           self.weight.add(self.noisy_weight.mul(weight_noise)),\n",
    "                           None\n",
    "                       )\n",
    "            else:\n",
    "                return F.linear(\n",
    "                           input,\n",
    "                           self.weight.add(self.noisy_weight.mul(weight_noise)),\n",
    "                           self.bias.add(self.noisy_bias.mul(bias_noise))\n",
    "                       )\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}, initial_sigma={}, factorized={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None, self.initial_sigma, self.factorized\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_size, obs_len, num_features=16, n_atoms=N_ATOMS, linear_type=LINEAR, initial_sigma=INIT_SIGMA, factorized=FACTORIZED):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): Dimension of each action\n",
    "            num_features (int): Number of features in the state\n",
    "            n_atoms (int): number of support atoms\n",
    "            linear_type (str): type of linear layers ('linear', 'noisy')\n",
    "            initial_sigma (float): initial weight value for noise parameters\n",
    "                when using noisy linear layers\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.obs_len = obs_len\n",
    "        self.num_features = num_features\n",
    "        self.n_atoms = n_atoms\n",
    "        self.linear_type = linear_type.lower()\n",
    "        self.factorized = bool(factorized)\n",
    "\n",
    "        def noisy_layer(in_features, out_features):\n",
    "            return NoisyLinear(in_features, out_features, True, initial_sigma, factorized)\n",
    "        linear = {'linear': nn.Linear, 'noisy': noisy_layer}[self.linear_type]\n",
    "\n",
    "        self.init_hidden = nn.Parameter(data=torch.randn(2, 1, 64), requires_grad=True)\n",
    "        self.init_cell   = nn.Parameter(data=torch.randn(2, 1, 64), requires_grad=True)\n",
    "\n",
    "        # N * obs_len * num_features\n",
    "        # x.view(-1, -1, x.size(-1))\n",
    "        # (N * obs_len) * num_features\n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Linear(num_features, 64)\n",
    "            )\n",
    "        # x.view(N, obs_len, x.size(-1))\n",
    "        # N * obs_len * 64\n",
    "        self.lstm  = nn.LSTM(64, 64, batch_first=True, bidirectional=True)\n",
    "        # N * obs_len * 128\n",
    "        self.inner_layers = nn.Sequential(\n",
    "            AttentionFFLayer(128),\n",
    "            AttentionFFLayer(128),\n",
    "            AttentionFFLayer(128),\n",
    "            FinalFFAttentionLayer(128)\n",
    "        )\n",
    "        # N * 128\n",
    "        self.fc_s = nn.Sequential(\n",
    "            linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            linear(64, n_atoms)\n",
    "        )\n",
    "        self.fc_a = nn.Sequential(\n",
    "            linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            linear(64, action_size * n_atoms)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc0(x.view(-1, x.size(-1))).view(x.size(0), x.size(1), -1)\n",
    "        x, _ = self.lstm(x, (self.init_hidden.repeat(1, x.size(0), 1), self.init_cell.repeat(1, x.size(0), 1)))\n",
    "\n",
    "        x = self.inner_layers(x)\n",
    "\n",
    "        state_value = self.fc_s(x)\n",
    "\n",
    "        advantage_values = self.fc_a(x)\n",
    "        advantage_values = advantage_values.view(advantage_values.size()[:-1] + (self.action_size, self.n_atoms))\n",
    "\n",
    "        dist_weights = state_value.unsqueeze(dim=-2) + advantage_values - advantage_values.mean(dim=-2, keepdim=True)\n",
    "\n",
    "        return dist_weights\n",
    "\n",
    "    def noise(self, enable):\n",
    "        enable = bool(enable)\n",
    "        for m in self.children():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.noise = enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, obs_len, num_features=16, seed=SEED, batch_size=BATCH_SIZE,\n",
    "                 buffer_size=BUFFER_SIZE, start_since=START_SINCE, gamma=GAMMA, target_update_every=T_UPDATE,\n",
    "                 tau=TAU, lr=LR, weight_decay=WEIGHT_DECAY, update_every=UPDATE_EVERY, priority_eps=P_EPS,\n",
    "                 a=A, initial_beta=INIT_BETA, n_multisteps=N_STEPS,\n",
    "                 v_min=V_MIN, v_max=V_MAX, clip=CLIP, n_atoms=N_ATOMS,\n",
    "                 initial_sigma=INIT_SIGMA, linear_type=LINEAR, factorized=FACTORIZED, **kwds):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            obs_len(int)\n",
    "            num_features (int): number of features in the state\n",
    "            seed (int): random seed\n",
    "            batch_size (int): size of each sample batch\n",
    "            buffer_size (int): size of the experience memory buffer\n",
    "            start_since (int): number of steps to collect before start training\n",
    "            gamma (float): discount factor\n",
    "            target_update_every (int): how often to update the target network\n",
    "            tau (float): target network soft-update parameter\n",
    "            lr (float): learning rate\n",
    "            weight_decay (float): weight decay for optimizer\n",
    "            update_every (int): update(learning and target update) interval\n",
    "            priority_eps (float): small base value for priorities\n",
    "            a (float): priority exponent parameter\n",
    "            initial_beta (float): initial importance-sampling weight\n",
    "            n_multisteps (int): number of steps to consider for each experience\n",
    "            v_min (float): minimum reward support value\n",
    "            v_max (float): maximum reward support value\n",
    "            clip (float): gradient norm clipping (`None` to disable)\n",
    "            n_atoms (int): number of atoms in the discrete support distribution\n",
    "            initial_sigma (float): initial noise parameter weights\n",
    "            linear_type (str): one of ('linear', 'noisy'); type of linear layer to use\n",
    "            factorized (bool): whether to use factorized gaussian noise in noisy layers\n",
    "        \"\"\"\n",
    "        if kwds != {}:\n",
    "            print(\"Ignored keyword arguments: \", end='')\n",
    "            print(*kwds, sep=', ')\n",
    "        assert isinstance(action_size, int)\n",
    "        assert isinstance(obs_len, int)\n",
    "        assert isinstance(num_features, int)\n",
    "        assert isinstance(seed, int)\n",
    "        assert isinstance(batch_size, int) and batch_size > 0\n",
    "        assert isinstance(buffer_size, int) and buffer_size >= batch_size\n",
    "        assert isinstance(start_since, int) and batch_size <= start_since <= buffer_size\n",
    "        assert isinstance(gamma, (int, float)) and 0 <= gamma <= 1\n",
    "        assert isinstance(target_update_every, int) and target_update_every > 0\n",
    "        assert isinstance(tau, (int, float)) and 0 <= tau <= 1\n",
    "        assert isinstance(lr, (int, float)) and lr >= 0\n",
    "        assert isinstance(weight_decay, (int, float)) and weight_decay >= 0\n",
    "        assert isinstance(update_every, int) and update_every > 0\n",
    "        assert isinstance(priority_eps, (int, float)) and priority_eps >= 0\n",
    "        assert isinstance(a, (int, float)) and 0 <= a <= 1\n",
    "        assert isinstance(initial_beta, (int, float)) and 0 <= initial_beta <= 1\n",
    "        assert isinstance(n_multisteps, int) and n_multisteps > 0\n",
    "        assert isinstance(v_min, (int, float)) and isinstance(v_max, (int, float)) and v_min < v_max\n",
    "        if clip: assert isinstance(clip, (int, float)) and clip >= 0\n",
    "        assert isinstance(n_atoms, int) and n_atoms > 0\n",
    "        assert isinstance(initial_sigma, (int, float)) and initial_sigma >= 0\n",
    "        assert isinstance(linear_type, str) and linear_type.strip().lower() in ('linear', 'noisy')\n",
    "        assert isinstance(factorized, bool)\n",
    "\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        self.action_size         = action_size\n",
    "        self.obs_len             = obs_len\n",
    "        self.num_features        = num_features\n",
    "        self.seed                = seed\n",
    "        self.batch_size          = batch_size\n",
    "        self.buffer_size         = buffer_size\n",
    "        self.start_since         = start_since\n",
    "        self.gamma               = gamma\n",
    "        self.target_update_every = target_update_every\n",
    "        self.tau                 = tau\n",
    "        self.lr                  = lr\n",
    "        self.weight_decay        = weight_decay\n",
    "        self.update_every        = update_every\n",
    "        self.priority_eps        = priority_eps\n",
    "        self.a                   = a\n",
    "        self.beta                = initial_beta\n",
    "        self.n_multisteps        = n_multisteps\n",
    "        self.v_min               = v_min\n",
    "        self.v_max               = v_max\n",
    "        self.clip                = clip\n",
    "        self.n_atoms             = n_atoms\n",
    "        self.initial_sigma       = initial_sigma\n",
    "        self.linear_type         = linear_type.strip().lower()\n",
    "        self.factorized          = factorized\n",
    "\n",
    "        # Distribution\n",
    "        self.supports = torch.linspace(v_min, v_max, n_atoms, device=device)\n",
    "        self.delta_z  = (v_max - v_min) / (n_atoms - 1)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local  = QNetwork(action_size, obs_len, num_features, n_atoms, linear_type, initial_sigma, factorized).to(device)\n",
    "        self.qnetwork_target = QNetwork(action_size, obs_len, num_features, n_atoms, linear_type, initial_sigma, factorized).to(device)\n",
    "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(buffer_size, batch_size, n_multisteps, gamma, a, False)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps and TARGET_UPDATE_EVERY steps)\n",
    "        self.u_step = 0\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        #  experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.u_step = (self.u_step + 1) % self.update_every\n",
    "        if self.u_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) >= self.start_since:\n",
    "                experiences, target_discount, is_weights, indices = self.memory.sample(self.beta)\n",
    "                new_priorities = self.learn(experiences, is_weights, target_discount)\n",
    "                self.memory.update_priorities(indices, new_priorities)\n",
    "\n",
    "        # update the target network every TARGET_UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.target_update_every\n",
    "        if self.t_step == 0:\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        if self.qnetwork_local.training:\n",
    "            switched = True\n",
    "            self.qnetwork_local.eval()\n",
    "        else:\n",
    "            switched = False\n",
    "        with torch.no_grad():\n",
    "            z_probs       = F.softmax(self.qnetwork_local(state), dim=-1)\n",
    "            action_values = self.supports.mul(z_probs).sum(dim=-1, keepdim=False)\n",
    "        if switched:\n",
    "            self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, is_weights, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            is_weights (torch.Tensor): tensor of importance-sampling weights\n",
    "            gamma (float): discount factor for the target max-Q value\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "            new_priorities (List[float]): list of new priority values for the given sample\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rows         = tuple(range(next_states.size(0)))\n",
    "            a_argmax     = F.softmax(self.qnetwork_local(next_states), dim=2)\\\n",
    "                               .mul(self.supports)\\\n",
    "                               .sum(dim=2, keepdim=False)\\\n",
    "                               .argmax(dim=1, keepdim=False)\n",
    "            p            = F.softmax(self.qnetwork_target(next_states)[rows, a_argmax], dim=1)\n",
    "            tz_projected = torch.clamp(rewards + (1 - dones) * gamma * self.supports, min=self.v_min, max=self.v_max)\n",
    "            # \"\"\"\n",
    "            b            = (tz_projected - self.v_min) / self.delta_z\n",
    "            u            = b.ceil()\n",
    "            l            = b.floor()\n",
    "            u_updates    = b - l + u.eq(l).type(u.dtype) # fixes the problem when having b == u == l\n",
    "            l_updates    = u - b\n",
    "            indices_flat = torch.cat((u.long(), l.long()), dim=1)\n",
    "            indices_flat = indices_flat.add(\n",
    "                               torch.arange(start=0,\n",
    "                                            end=b.size(0) * b.size(1),\n",
    "                                            step=b.size(1),\n",
    "                                            dtype=indices_flat.dtype,\n",
    "                                            layout=indices_flat.layout,\n",
    "                                            device=indices_flat.device).unsqueeze(1)\n",
    "                           ).view(-1)\n",
    "            updates_flat = torch.cat((u_updates.mul(p), l_updates.mul(p)), dim=1).view(-1)\n",
    "            target_distributions = torch.zeros_like(p)\n",
    "            target_distributions.view(-1).index_add_(0, indices_flat, updates_flat)\n",
    "\n",
    "\n",
    "        pred_distributions = self.qnetwork_local(states)\n",
    "        pred_distributions = pred_distributions.gather(dim=1, index=actions.view(-1, 1, 1).expand(-1, -1, pred_distributions.size(2))).squeeze(1)\n",
    "\n",
    "        \"\"\"\n",
    "        cross_entropy = target_distributions.mul(pred_distributions.exp().sum(dim=-1, keepdim=True).log() - pred_distributions).sum(dim=-1, keepdim=False)\n",
    "        new_priorities = cross_entropy.detach().add(self.priority_eps).cpu().numpy()\n",
    "        loss = cross_entropy.mul(is_weights.view(-1)).mean()\n",
    "        \"\"\"\n",
    "        kl_divergence = F.kl_div(F.log_softmax(pred_distributions, dim=-1), target_distributions, reduce=False).sum(dim=-1, keepdim=False)\n",
    "        new_priorities = kl_divergence.detach().add(self.priority_eps).cpu().numpy()\n",
    "        loss = kl_divergence.mul(is_weights.view(-1)).mean()\n",
    "#         \"\"\"\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if self.clip:\n",
    "            torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(), self.clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return new_priorities\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.qnetwork_local  = self.qnetwork_local.to(device)\n",
    "        self.qnetwork_target = self.qnetwork_target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, n_multisteps, gamma, a, separate_experiences):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            n_multisteps (int): number of time steps to consider for each experience\n",
    "            gamma (float): discount factor\n",
    "            a (float): priority exponent parameter\n",
    "            separate_experiences (bool): whether to store experiences with no overlap\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_multisteps = n_multisteps\n",
    "        self.gamma = gamma\n",
    "        self.a = a\n",
    "        self.separate_experiences = bool(separate_experiences)\n",
    "\n",
    "        self.memory_write_idx = 0\n",
    "        self._non_leaf_depth = math.ceil(math.log2(buffer_size))\n",
    "        self._memory_start_idx = 2 ** self._non_leaf_depth\n",
    "        self._buffer_is_full = False\n",
    "        self.memory = [None for _ in range(buffer_size)]\n",
    "        self.priorities_a = np.zeros(buffer_size)\n",
    "        self.tree = np.zeros(self._memory_start_idx + buffer_size) # starts from index 1, not 0; makes implementation easier and reduces many small computations\n",
    "\n",
    "        self.multistep_collector = deque(maxlen=n_multisteps)\n",
    "        self.max_priority_a = 1.\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "        self._divisors = np.power(2, np.arange(1, self._non_leaf_depth + 1)).reshape((-1, 1))\n",
    "        self._discounts = np.power(self.gamma, np.arange(self.n_multisteps + 1))\n",
    "        self._target_discount = float(self._discounts[-1])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.multistep_collector.append(e)\n",
    "        if len(self.multistep_collector) == self.n_multisteps:\n",
    "            self.memory[self.memory_write_idx] = tuple(self.multistep_collector)\n",
    "\n",
    "            delta_priority_a = self.max_priority_a - self.priorities_a[self.memory_write_idx]\n",
    "            tree_idx = self._memory_start_idx + self.memory_write_idx\n",
    "            self.priorities_a[self.memory_write_idx] = self.max_priority_a\n",
    "            self.tree[tree_idx] = self.max_priority_a\n",
    "            # tree_indices = np.floor_divide(tree_idx, self._divisors).reshape((-1,))\n",
    "            # np.add.at(self.tree, tree_indices, np.tile(delta_priority_a, self._non_leaf_depth))\n",
    "            for _ in range(self._non_leaf_depth):\n",
    "                tree_idx = tree_idx // 2\n",
    "                self.tree[tree_idx] += delta_priority_a\n",
    "\n",
    "            self.memory_write_idx += 1\n",
    "            if self.memory_write_idx >= self.buffer_size:\n",
    "                self._buffer_is_full = True\n",
    "                self.memory_write_idx = 0\n",
    "\n",
    "            if self.separate_experiences:\n",
    "                self.multistep_collector.clear()\n",
    "        if done:\n",
    "            self.multistep_collector.clear()\n",
    "\n",
    "    def sample(self, beta):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            beta (int or float): parameter used for calculating importance-priority weights\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            target_discount (float): discount factor for target max-Q value\n",
    "            is_weights (torch.Tensor): tensor of importance-sampling weights\n",
    "            indices (np.ndarray): sample indices\"\"\"\n",
    "        indices_not_prepared = True\n",
    "        sample_value_basis = np.linspace(0, self.tree[1], num=self.batch_size, endpoint=False, dtype=np.float32)\n",
    "        while indices_not_prepared:\n",
    "            sample_values = np.add(sample_value_basis, np.multiply(np.random.rand(self.batch_size), sample_value_basis[1]))\n",
    "            tree_indices = np.ones(self.batch_size, dtype=np.int32)\n",
    "            try:\n",
    "                for _ in range(self._non_leaf_depth):\n",
    "                    left_child_indices = np.multiply(tree_indices, 2)\n",
    "                    right_child_indices = np.add(left_child_indices, 1)\n",
    "                    greater_than_left = np.greater(sample_values, self.tree[left_child_indices])\n",
    "                    sample_values = np.where(greater_than_left, np.subtract(sample_values, self.tree[left_child_indices]), sample_values)\n",
    "                    tree_indices = np.where(greater_than_left, right_child_indices, left_child_indices)\n",
    "            except IndexError: # Don't know exactly why it occurs. Suspecting numerical error issues with floating numbers as a probable cause\n",
    "                continue\n",
    "            else:\n",
    "                indices_not_prepared = False\n",
    "        indices = np.subtract(tree_indices, self._memory_start_idx)\n",
    "\n",
    "        experiences = tuple(zip(*[self.memory[i] for i in indices if self.memory[i] is not None]))\n",
    "\n",
    "        first_states = torch.tensor([e[0] for e in experiences[0]], dtype=torch.float, device=device)\n",
    "        actions      = torch.tensor([e[1] for e in experiences[0]], dtype=torch.long, device=device)\n",
    "        rewards      = torch.tensor(\n",
    "                           np.sum(\n",
    "                               np.multiply(\n",
    "                                   np.array([[e[2] for e in experiences_step] for experiences_step in experiences]).transpose(), self._discounts[:-1]\n",
    "                               ), axis=1, keepdims=True\n",
    "                           ), dtype=torch.float, device=device)\n",
    "        last_states  = torch.tensor([e[3] for e in experiences[-1]], dtype=torch.float, device=device)\n",
    "        dones        = torch.tensor([e[4] for e in experiences[-1]], dtype=torch.float, device=device).view(-1, 1)\n",
    "\n",
    "        is_weights = np.divide(self.priorities_a[indices], self.tree[1])\n",
    "        is_weights = np.power(np.multiply(is_weights, self.buffer_size if self._buffer_is_full else self.memory_write_idx), -beta)\n",
    "        is_weights = torch.tensor(np.divide(is_weights, np.max(is_weights)).reshape((-1, 1)), dtype=torch.float, device=device)\n",
    "\n",
    "        return (first_states, actions, rewards, last_states, dones), self._target_discount, is_weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        \"\"\"Update the priorities for the experiences of given indices to the given new values.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            indices (array_like): indices of experience priorities to update\n",
    "            new_priorities (array-like): new priority values for given indices\"\"\"\n",
    "        # Remove Duplicate Samples (discard except the first occurence in the array)\n",
    "        indices, idx_indices = np.unique(indices, return_index=True)\n",
    "        new_priorities_a = np.power(new_priorities[idx_indices], self.a)\n",
    "\n",
    "        delta_priority_a = np.subtract(new_priorities_a, self.priorities_a[indices])\n",
    "        tree_indices = np.add(indices, self._memory_start_idx)\n",
    "        self.priorities_a[indices] = new_priorities_a\n",
    "        self.tree[tree_indices] = new_priorities_a\n",
    "        tree_indices = np.floor_divide(tree_indices, self._divisors).reshape((-1,))\n",
    "        np.add.at(self.tree, tree_indices, np.tile(delta_priority_a, self._non_leaf_depth))\n",
    "\n",
    "        self.max_priority_a = np.max(self.priorities_a)\n",
    "\n",
    "    def reset_multisteps(self):\n",
    "        self.multistep_collector.clear()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return self.buffer_size if self._buffer_is_full else self.memory_write_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey/Binanace_trading_simulation/TradingGym/AttentionTest/custom_trading_env.py:99: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.price = self.df_sample[self.price_name].as_matrix()\n",
      "/home/jeffrey/Binanace_trading_simulation/TradingGym/AttentionTest/custom_trading_env.py:101: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.obs_features = self.df_sample[self.using_feature].as_matrix()\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "\n",
    "agent = Agent(action_size=2 * n_action_intervals + 1, obs_len=obs_data_len, num_features=env.reset().shape[-1], **hyperparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.4\n",
    "beta_inc = (1 - beta) / 1000\n",
    "agent.beta = beta\n",
    "scores_list = []\n",
    "loss_list = []\n",
    "n_epi = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey/Binanace_trading_simulation/TradingGym/AttentionTest/custom_trading_env.py:99: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.price = self.df_sample[self.price_name].as_matrix()\n",
      "/home/jeffrey/Binanace_trading_simulation/TradingGym/AttentionTest/custom_trading_env.py:101: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.obs_features = self.df_sample[self.using_feature].as_matrix()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode: 1, avg score: -0.1267\n",
      "  Actions: [2 2 4 ... 2 2 2]\n",
      "# of episode: 2, avg score: -0.1435\n",
      "  Actions: [2 6 2 ... 2 4 4]\n",
      "# of episode: 3, avg score: -0.1267\n",
      "  Actions: [2 2 2 ... 7 5 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey/anaconda3/envs/RL/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode: 4, avg score: -0.2716\n",
      "  Actions: [ 2  4  2 ...  2 10 10]\n",
      "# of episode: 5, avg score: 0.0439\n",
      "  Actions: [ 3  3  6 ... 10  5  7]\n",
      "# of episode: 6, avg score: -0.0513\n",
      "  Actions: [0 3 0 ... 0 0 0]\n",
      "# of episode: 7, avg score: 0.2081\n",
      "  Actions: [0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-05d85f3b6d1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_action_intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-e5bcb9b48ad3>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_since\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_discount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mnew_priorities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_discount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_priorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-e5bcb9b48ad3>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, is_weights, gamma)\u001b[0m\n\u001b[1;32m    176\u001b[0m                                \u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                                \u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mp\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_argmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mtz_projected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "# for n_epi in range(10000):  # 게임 1만판 진행\n",
    "for i_episode in range(n_episodes):\n",
    "    n_epi +=1\n",
    "\n",
    "    if (i_episode + 1) % 500 == 0:\n",
    "        sample_len += 480\n",
    "        env.sample_len = sample_len\n",
    "\n",
    "    state = env.reset()\n",
    "    score = 0.\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    # for t in range(num_steps):\n",
    "    while True:\n",
    "        action = int(agent.act(state, eps=0.))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        score += reward\n",
    "        if reward < 0:\n",
    "            reward *= risk_aversion_multiplier\n",
    "        if sell_at_end and done:\n",
    "            action = 2 * n_action_intervals\n",
    "        actions.append(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    else:\n",
    "        agent.memory.reset_multisteps()\n",
    "\n",
    "    beta = min(1, beta + beta_inc)\n",
    "    agent.beta = beta\n",
    "\n",
    "    scores_list.append(score)\n",
    "\n",
    "    if n_epi % print_interval == 0 and n_epi != 0:\n",
    "        print_str = \"# of episode: {:d}, avg score: {:.4f}\\n  Actions: {}\".format(n_epi, sum(scores_list[-print_interval:]) / print_interval, np.array(actions))\n",
    "        print(print_str)\n",
    "#         with open(os.path.join(save_location, \"output_log.txt\"), mode='a') as f:\n",
    "#             f.write(print_str + '\\n')\n",
    "\n",
    "#     if n_epi % save_interval == 0:\n",
    "#         torch.save(agent.qnetwork_local.state_dict(), os.path.join(save_location, 'TradingGym_Rainbow_{:d}.pth'.format(n_epi)))\n",
    "#         torch.save(scores_list, os.path.join(save_location, 'scores.pth'))\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
